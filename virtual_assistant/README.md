# Impl


## ğŸ“ **DescriÃ§Ã£o**
Este projeto apresenta dois experimentos de implementaÃ§Ã£o de Modelos de Linguagem de Grande Escala (LLMs) em uma mÃ¡quina local.  

- **Hugging Face:** O modelo **Meta-Llama-3-8B-Instruct** foi acessado atravÃ©s da API do Hugging Face Hub, utilizando o LangChain para facilitar a integraÃ§Ã£o. Aqui, o modelo roda na nuvem da Hugging Face, sem necessidade de processamento local.  
- **Ollama:** O modelo **Phi-3** foi executado **localmente** atravÃ©s do Ollama, permitindo inferÃªncia sem depender de servidores externos.  

- Em suma, a principal diferenÃ§a entre os experimentos Ã© que o Hugging Face utiliza a **API na nuvem**, enquanto o Ollama permite rodar o modelo **diretamente na mÃ¡quina local**, garantindo mais privacidade e independÃªncia de servidores externos.  

---

## **Ficha TÃ©cnica**

| ğŸ” **Item**           | ğŸ“„ **DescriÃ§Ã£o** |
|---------------------|----------------|
| **ğŸ› ï¸ Tecnologias** | Python, LangChain, Ollama, Hugging Face Hub |
| **ğŸ“¦ DependÃªncias** | `langchain`, `langchain_community`, `langchain-huggingface`, `langchain_ollama`, `python-dotenv` |
| **âš™ï¸ Funcionalidade** | ComparaÃ§Ã£o entre execuÃ§Ã£o de LLMs na nuvem (Hugging Face) e localmente (Ollama) |
| **ğŸ“Œ Modelos Utilizados** | `meta-llama/Meta-Llama-3-8B-Instruct` (Hugging Face) e `phi3` (Ollama) |
| **ğŸŒ ExecuÃ§Ã£o na Nuvem** | Hugging Face Hub (requisiÃ§Ã£o via API) |
| **ğŸ’» ExecuÃ§Ã£o Local** | Ollama (modelo rodando no prÃ³prio hardware) |
| **ğŸ”‘ AutenticaÃ§Ã£o** | API Key do Hugging Face via `.env` |


